{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "134e0bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.41.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: keras~=2.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: gast==0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: clang~=5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.14.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow) (1.5.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (49.6.0.post20210108)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c7088cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Private</th>\n",
       "      <th>Apps</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Top10perc</th>\n",
       "      <th>Top25perc</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "      <th>Outstate</th>\n",
       "      <th>Room.Board</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>S.F.Ratio</th>\n",
       "      <th>perc.alumni</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abilene Christian University</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1660</td>\n",
       "      <td>1232</td>\n",
       "      <td>721</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>2885</td>\n",
       "      <td>537</td>\n",
       "      <td>7440</td>\n",
       "      <td>3300</td>\n",
       "      <td>450</td>\n",
       "      <td>2200</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7041</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Private  Apps  Accept  Enroll  Top10perc  \\\n",
       "Abilene Christian University     Yes  1660    1232     721         23   \n",
       "\n",
       "                              Top25perc  F.Undergrad  P.Undergrad  Outstate  \\\n",
       "Abilene Christian University         52         2885          537      7440   \n",
       "\n",
       "                              Room.Board  Books  Personal  PhD  Terminal  \\\n",
       "Abilene Christian University        3300    450      2200   70        78   \n",
       "\n",
       "                              S.F.Ratio  perc.alumni  Expend  Grad.Rate  \n",
       "Abilene Christian University       18.1           12    7041         60  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# (a)\n",
    "# Defining the s3 bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'gabrielferreira-data-455-bucket'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "# Defining the file to be read from s3 bucket\n",
    "file_key = \"College.csv\"\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "file_object = bucket_object.get()\n",
    "file_content_stream = file_object.get('Body')\n",
    "\n",
    "# Reading the csv file\n",
    "college = pd.read_csv(file_content_stream)\n",
    "college.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dac86236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes    565\n",
       "No     212\n",
       "Name: Private, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college['Private'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4efe5b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    565\n",
       "0    212\n",
       "Name: Private, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (b)\n",
    "# Changin Private variable from a categorical to a numerical variable\n",
    "college['Private'] = pd.Series(np.where(college['Private'].values == 'Yes', 1, 0), college.index)\n",
    "college['Private'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f64d8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c)\n",
    "# Defining predictors and target variables\n",
    "X = college[['Private', 'F.Undergrad', 'P.Undergrad', 'Outstate', 'Room.Board', 'Books', 'Personal', 'S.F.Ratio', 'Grad.Rate']]\n",
    "Y = college['Apps']\n",
    "\n",
    "# Splitting data into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "# (d)\n",
    "# Trasforming the predictors variables in the train and test dataset to 0-1 scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e244f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9447995.576057166"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (e)\n",
    "# Building a multiple linear regression model\n",
    "lm_md = LinearRegression().fit(X_train, Y_train)\n",
    "\n",
    "# Prediciting on test dataset\n",
    "lm_md_preds = lm_md.predict(X_test)\n",
    "\n",
    "# Computing the mse\n",
    "lm_md_mse = np.mean(np.power(lm_md_preds - Y_test, 2))\n",
    "lm_md_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f028719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (f)\n",
    "ridge_cv = RidgeCV(np.linspace(0.001, 100, num = 100), cv = 5).fit(X_train, Y_train)\n",
    "rigde_alpha = ridge_cv.alpha_\n",
    "rigde_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e24a712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9447465.45391115"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building Ridge regression model\n",
    "ridge_md = Ridge(alpha = rigde_alpha).fit(X_train, Y_train)\n",
    "\n",
    "# Predicting on test dataset\n",
    "ridge_md_preds = ridge_md.predict(X_test)\n",
    "\n",
    "# Computing the mse\n",
    "ridge_md_mse = np.mean(np.power(ridge_md_preds - Y_test, 2))\n",
    "ridge_md_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edd1c15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.011090909090909"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (g)\n",
    "lasso_cv = LassoCV(alphas = np.linspace(0.001, 100, num = 100), normalize = True, cv = 5).fit(X_train, Y_train)\n",
    "lasso_alpha = lasso_cv.alpha_\n",
    "lasso_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72b3461c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9307336.589064173"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building Lasso regression model\n",
    "lasso_md = Lasso(alpha = lasso_alpha).fit(X_train, Y_train)\n",
    "\n",
    "# Predicting on test dataset\n",
    "lasso_md_preds = lasso_md.predict(X_test)\n",
    "\n",
    "# Computing the mse\n",
    "lasso_md_mse = np.mean(np.power(lasso_md_preds - Y_test, 2))\n",
    "lasso_md_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1e3d5",
   "metadata": {},
   "source": [
    "## (h)\n",
    "#### Using the results from parts (e), (f) and (g), I would use the model from part (e) to predict the number of applications that a university receive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95816f99",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c665a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a)\n",
    "# Train dataset\n",
    "# Defining the s3 bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'gabrielferreira-data-455-bucket'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "# Defining the file to be read from s3 bucket\n",
    "file_key = \"churn-bigml-80.csv\"\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "file_object = bucket_object.get()\n",
    "file_content_stream = file_object.get('Body')\n",
    "\n",
    "# Reading the csv file\n",
    "telecom_train = pd.read_csv(file_content_stream)\n",
    "\n",
    "# Test dataset\n",
    "# Defining the s3 bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'gabrielferreira-data-455-bucket'\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "# Defining the file to be read from s3 bucket\n",
    "file_key = \"churn-bigml-20.csv\"\n",
    "\n",
    "bucket_object = bucket.Object(file_key)\n",
    "file_object = bucket_object.get()\n",
    "file_content_stream = file_object.get('Body')\n",
    "\n",
    "# Reading the csv file\n",
    "telecom_test = pd.read_csv(file_content_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc1a39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b)\n",
    "telecom_train['Churn'] = np.where(telecom_train['Churn'] == True, 1, 0)\n",
    "telecom_test['Churn'] = np.where(telecom_test['Churn'] == True, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cef05400",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_train['International_plan'] = np.where(telecom_train['International_plan'] == \"Yes\", 1, 0)\n",
    "telecom_test['International_plan'] = np.where(telecom_test['International_plan'] == \"Yes\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee009174",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_train['Voice_mail_plan'] = np.where(telecom_train['Voice_mail_plan'] == \"Yes\", 1, 0)\n",
    "telecom_test['Voice_mail_plan'] = np.where(telecom_test['Voice_mail_plan'] == \"Yes\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5fde199",
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_train['Total_charge'] = np.sum(telecom_train[['Total_day_charge', 'Total_eve_charge', 'Total_night_charge', 'Total_intl_charge']], axis = 1)\n",
    "telecom_test['Total_charge'] = np.sum(telecom_test[['Total_day_charge', 'Total_eve_charge', 'Total_night_charge', 'Total_intl_charge']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a57b6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c)\n",
    "telecom_train = telecom_train[['Account_length', 'International_plan', 'Voice_mail_plan', 'Total_charge', 'Customer_service_calls', 'Churn']]\n",
    "telecom_test = telecom_test[['Account_length', 'International_plan', 'Voice_mail_plan', 'Total_charge', 'Customer_service_calls', 'Churn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5927b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06851413,  0.31353904, -0.07166731,  0.5090659 ,  0.47444084])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (d)\n",
    "# Defining predictors and target variables\n",
    "X = telecom_train[['Account_length', 'International_plan', 'Voice_mail_plan', 'Total_charge', 'Customer_service_calls']]\n",
    "Y = telecom_train['Churn']\n",
    "\n",
    "# (1)\n",
    "# Splitting the data into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify = Y)\n",
    "\n",
    "# (2)\n",
    "# Transfomating all input variables to 0-1 scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# (3)\n",
    "# Estimating the optimal lambda for LASSO using default values.\n",
    "lasso_cv = LassoCV(normalize = True, cv = 5).fit(X_train, Y_train)\n",
    "lasso_cv = lasso_cv.alpha_\n",
    "\n",
    "lasso_md = Lasso(alpha = lasso_cv, normalize = True).fit(X_train, Y_train)\n",
    "lasso_md.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fc5d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list to store the results\n",
    "lasso_results = list()\n",
    "\n",
    "for i in range(0,1000):\n",
    "    X = telecom_train[['Account_length', 'International_plan', 'Voice_mail_plan', 'Total_charge', 'Customer_service_calls']]\n",
    "    Y = telecom_train['Churn']\n",
    "\n",
    "    # (1)\n",
    "    # Splitting the data into train and test\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify = Y)\n",
    "\n",
    "    # (2)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    # (3)\n",
    "    # Estimating the optimal lambda for LASSO using default values.\n",
    "    lasso_cv = LassoCV(normalize = True, cv = 5).fit(X_train, Y_train)\n",
    "    lasso_cv = lasso_cv.alpha_\n",
    "\n",
    "    lasso_md = Lasso(alpha = lasso_cv, normalize = True).fit(X_train, Y_train)\n",
    "    lasso_results.append(lasso_md.coef_)\n",
    "    \n",
    "lasso_results = pd.DataFrame(lasso_results, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a30e7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable index:  0\n",
      "Number of 0 in this variable:  269\n",
      " \n",
      "Variable index:  1\n",
      "Number of 0 in this variable:  0\n",
      " \n",
      "Variable index:  2\n",
      "Number of 0 in this variable:  0\n",
      " \n",
      "Variable index:  3\n",
      "Number of 0 in this variable:  0\n",
      " \n",
      "Variable index:  4\n",
      "Number of 0 in this variable:  0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Counting coefficients equal zero\n",
    "for i in range(0, len(lasso_results.iloc[0,:])):\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for j in range(0, len(lasso_results.iloc[:,0])):\n",
    "        \n",
    "        if lasso_results.iloc[j, i] == 0:\n",
    "            count = count + 1\n",
    "    \n",
    "    print(\"Variable index: \", i)\n",
    "    print('Number of 0 in this variable: ', count)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a44596b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping variables whose estimated coefficients is 0 more than 200 times.\n",
    "telecom_train = telecom_train.drop('Account_length', axis = 1)\n",
    "telecom_test = telecom_test.drop('Account_length', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00cec5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (e)\n",
    "# Defining predictors and target variables\n",
    "X_telecom_train = telecom_train[['International_plan', 'Voice_mail_plan', 'Total_charge', 'Customer_service_calls']]\n",
    "Y_telecom_train = telecom_train['Churn']\n",
    "\n",
    "# (i)\n",
    "# importing StratifiedKFold function\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "# Setting StratifiedKFold with 5 folds\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_ix, test_ix in skf.split(X_telecom_train, Y_telecom_train):\n",
    "    \n",
    "     ## Splitting data into train and test\n",
    "    X_train, X_test = X_telecom_train.loc[train_ix], X_telecom_train.loc[test_ix]\n",
    "    Y_train, Y_test = Y_telecom_train.loc[train_ix], Y_telecom_train.loc[test_ix]\n",
    "    \n",
    "    # (ii)\n",
    "    ## Transforming all input variables to 0-1 scale\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "468c1561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8974358974358975"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a multi-layer perceptron (MLP) model with one single hidden layer with 5 neurons (hyperbolic \n",
    "# tangent as the activation function) and softmax as the activation function for the output.\n",
    "md1 = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(5, input_dim = 4, activation = 'tanh'),\n",
    "      tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    " # Use the stochastic descent gradient as the method to estimate the weights (optimizer = ’sgd’, \n",
    "# loss = ’categorical crossentropy’, and metrics = [’accuracy’]). Use epochs = 100 and \n",
    "    # batch size = 100 to build the model. \n",
    "md1.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "md1.fit(X_train,\n",
    "        tf.keras.utils.to_categorical(Y_train, num_classes = 2), \n",
    "        epochs = 100, \n",
    "        batch_size =100, \n",
    "        verbose = 0,\n",
    "        validation_data = (X_test, tf.keras.utils.to_categorical(Y_test, num_classes = 2)))\n",
    "        \n",
    "# using the model to predict on the test dataset. Using 10% as the cut-off value, compute the recall of this model. \n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Getting predictions from multi-layer perceptron (MLP) model on X_test dataset\n",
    "preds = md1.predict(X_test)[:,1]\n",
    "\n",
    "# Defining likelyhoods to labels\n",
    "preds = np.where(preds < 0.10, 0, 1)\n",
    "# Report the average recall score across the 5-folds\n",
    "recall_score(Y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd91bda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9615384615384616"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a multi-layer perceptron (MLP) model with one single hidden layer with 5 neurons (ReLU\n",
    "# as the activation function) and softmax as the activation function for the output.\n",
    "md2 = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(5, input_dim = 4, activation = 'relu'),\n",
    "      tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    " # Use the stochastic descent gradient as the method to estimate the weights (optimizer = ’sgd’, \n",
    "# loss = ’categorical crossentropy’, and metrics = [’accuracy’]). Use epochs = 100 and \n",
    "    # batch size = 100 to build the model. \n",
    "md2.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "md2.fit(X_train,\n",
    "        tf.keras.utils.to_categorical(Y_train, num_classes = 2), \n",
    "        epochs = 100, \n",
    "        batch_size =100, \n",
    "        verbose = 0,\n",
    "        validation_data = (X_test, tf.keras.utils.to_categorical(Y_test, num_classes = 2)))\n",
    "        \n",
    "# using the model to predict on the test dataset. Using 10% as the cut-off value, compute the recall of this model. \n",
    "# Getting predictions from multi-layer perceptron (MLP) model on X_test dataset\n",
    "preds_2 = md2.predict(X_test)[:,1]\n",
    "\n",
    "# Defining likelyhoods to labels\n",
    "preds_2 = np.where(preds_2 < 0.10, 0, 1)\n",
    "# Report the average recall score across the 5-folds\n",
    "recall_score(Y_test, preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e157245b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.782051282051282"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the support vector machine (SVM) with kernel = 'rbf'\n",
    "from sklearn.svm import SVR, SVC\n",
    "\n",
    "# Building the svm with kernel = 'rbf'\n",
    "md3 = SVC(kernel = 'rbf', probability = True).fit(X_train, Y_train)\n",
    "\n",
    "# Predicting on the test dataset\n",
    "preds_3 = md3.predict_proba(X_test)[:,1]\n",
    "preds_3 = np.where(preds_3 < 0.10, 0, 1)\n",
    "recall_score(Y_test, preds_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d7b24c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7692307692307693"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the support vector machine (SVM) with kernel = 'poly'\n",
    "md4 = SVC(kernel = 'poly', probability = True).fit(X_train, Y_train)\n",
    "\n",
    "# Predicting on the test dataset\n",
    "preds_4 = md4.predict_proba(X_test)[:,1]\n",
    "preds_4 = np.where(preds_4 < 0.10, 0, 1)\n",
    "recall_score(Y_test, preds_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (f)\n",
    "# Creating list to store recall scores provided by the models\n",
    "md1_recall = list()\n",
    "md2_recall = list()\n",
    "md3_recall = list()\n",
    "md4_recall = list()\n",
    "\n",
    "# Setting StratifiedKFold with 5 folds\n",
    "# skf = StratifiedKFold(n_splits = 5, shuffle = True) \n",
    "\n",
    "for i in range(0,100):\n",
    "    \n",
    "    # Setting StratifiedKFold with 5 folds\n",
    "    skf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "    \n",
    "    md1_kfold_result = list()\n",
    "    md2_kfold_result = list()\n",
    "    md3_kfold_result = list()\n",
    "    md4_kfold_result = list()\n",
    "    \n",
    "    for train_ix, test_ix in skf.split(X_telecom_train, Y_telecom_train):\n",
    "    \n",
    "         ## Splitting data into train and test\n",
    "        X_train, X_test = X_telecom_train.loc[train_ix], X_telecom_train.loc[test_ix]\n",
    "        Y_train, Y_test = Y_telecom_train.loc[train_ix], Y_telecom_train.loc[test_ix]\n",
    "    \n",
    "        # (ii)\n",
    "        ## Transforming all input variables to 0-1 scale\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        \n",
    "        # Building a multi-layer perceptron (MLP) model with one single hidden layer with 5 neurons (hyperbolic \n",
    "        # tangent as the activation function) and softmax as the activation function for the output.\n",
    "        md1 = tf.keras.models.Sequential([\n",
    "          tf.keras.layers.Dense(5, input_dim = 4, activation = 'tanh'),\n",
    "          tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "        ])\n",
    "\n",
    "        # Use the stochastic descent gradient as the method to estimate the weights (optimizer = ’sgd’, \n",
    "        # loss = ’categorical crossentropy’, and metrics = [’accuracy’]). Use epochs = 100 and \n",
    "        # batch size = 100 to build the model. \n",
    "        md1.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        md1.fit(X_train,\n",
    "            tf.keras.utils.to_categorical(Y_train, num_classes = 2), \n",
    "            epochs = 100, \n",
    "            batch_size =100, \n",
    "            verbose = 0,\n",
    "            validation_data = (X_test, tf.keras.utils.to_categorical(Y_test, num_classes = 2)))\n",
    "        \n",
    "        # using the model to predict on the test dataset. Using 10% as the cut-off value, compute the recall of this model. \n",
    "        # Getting predictions from multi-layer perceptron (MLP) model on X_test dataset\n",
    "        preds = md1.predict(X_test)[:,1]\n",
    "\n",
    "        # Defining likelyhoods to labels\n",
    "        preds = np.where(preds < 0.10, 0, 1)\n",
    "        # Report the average recall score across the 5-folds\n",
    "        md1_kfold_result.append(recall_score(Y_test, preds))\n",
    "        \n",
    " \n",
    "    \n",
    "        ##### Model 2\n",
    "        # Building a multi-layer perceptron (MLP) model with one single hidden layer with 5 neurons (ReLU\n",
    "        # as the activation function) and softmax as the activation function for the output.\n",
    "        md2 = tf.keras.models.Sequential([\n",
    "          tf.keras.layers.Dense(5, input_dim = 4, activation = 'relu'),\n",
    "          tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "        ])\n",
    "\n",
    "        # Use the stochastic descent gradient as the method to estimate the weights (optimizer = ’sgd’, \n",
    "        # loss = ’categorical crossentropy’, and metrics = [’accuracy’]). Use epochs = 100 and \n",
    "        # batch size = 100 to build the model. \n",
    "        md2.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        md2.fit(X_train,\n",
    "            tf.keras.utils.to_categorical(Y_train, num_classes = 2), \n",
    "            epochs = 100, \n",
    "            batch_size =100, \n",
    "            verbose = 0,\n",
    "            validation_data = (X_test, tf.keras.utils.to_categorical(Y_test, num_classes = 2)))\n",
    "        \n",
    "        # using the model to predict on the test dataset. Using 10% as the cut-off value, compute the recall of this model. \n",
    "        # Getting predictions from multi-layer perceptron (MLP) model on X_test dataset\n",
    "        preds_2 = md2.predict(X_test)[:,1]\n",
    "\n",
    "        # Defining likelyhoods to labels\n",
    "        preds_2 = np.where(preds_2 < 0.10, 0, 1)\n",
    "    \n",
    "        # Computing the average recall score across the 5-folds\n",
    "        md2_kfold_result.append(recall_score(Y_test, preds_2))\n",
    "        \n",
    "        ##### Model 3\n",
    "        # Building the support vector machine (SVM) with kernel = 'rbf'\n",
    "        md3 = SVC(kernel = 'rbf', probability = True).fit(X_train, Y_train)\n",
    "\n",
    "        # Predicting on the test dataset\n",
    "        preds_3 = md3.predict_proba(X_test)[:,1]\n",
    "    \n",
    "        # Defining likelyhoods to labels\n",
    "        preds_3 = np.where(preds_3 < 0.10, 0, 1)\n",
    "    \n",
    "        # Computing the average recall score across the 5-folds\n",
    "        md3_kfold_result.append(recall_score(Y_test, preds_3))\n",
    "    \n",
    "        #### Model 4\n",
    "        # Building the support vector machine (SVM) with kernel = 'poly'\n",
    "        md4 = SVC(kernel = 'poly', probability = True).fit(X_train, Y_train)\n",
    "\n",
    "        # Predicting on the test dataset\n",
    "        preds_4 = md4.predict_proba(X_test)[:,1]\n",
    "        preds_4 = np.where(preds_4 < 0.10, 0, 1)\n",
    "        md4_kfold_result.append(recall_score(Y_test, preds_4))\n",
    "    \n",
    "    ## Appending average recall of 5-fold cross validation\n",
    "    md1_recall.append(np.mean(md1_kfold_result))\n",
    "    \n",
    "    # Computing the mean of recall scores for model 2\n",
    "    md2_recall.append(np.mean(md2_kfold_result))\n",
    "    \n",
    "    # Computing the mean of recall scores for model 3\n",
    "    md3_recall.append(np.mean(md3_kfold_result))\n",
    "    \n",
    "    # Computing the mean of recall scores for model 4\n",
    "    md4_recall.append(np.mean(md4_kfold_result))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing model 1\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Plotting iteration results\n",
    "fig = plt.figure(figsize = (12, 8))\n",
    "\n",
    "iterations = range(0, 3)\n",
    "plt.plot(iterations, md1_recall, marker = 'o', color = 'blue',\n",
    "        label = 'First Model')\n",
    "plt.plot(iterations, md2_recall, marker = 'o', color = 'orange',\n",
    "        label = 'Second Model')\n",
    "plt.plot(iterations, md3_recall, marker = 'o', color = 'red',\n",
    "         label = 'Third Model')\n",
    "plt.plot(iterations, md4_recall, marker = 'o', color = 'black',\n",
    "        label = 'Fourth Model')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c07561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (g)\n",
    "# Defining predictors and target variables\n",
    "X_telecom_train = telecom_train[['International_plan', 'Voice_mail_plan', 'Total_charge', 'Customer_service_calls']]\n",
    "Y_telecom_train = telecom_train['Churn']\n",
    "X_telecom_test = telecom_test[['International_plan', 'Voice_mail_plan', 'Total_charge', 'Customer_service_calls']]\n",
    "Y_telecom_test = telecom_test['Churn']\n",
    "\n",
    "# Transforming the input variables to 0-1 scale \n",
    "X_telecom_train = scaler.fit_transform(telecom_train)\n",
    "X_telecom_test = scaler.fit_transform(telecom_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a multi-layer perceptron (MLP) model with one single hidden layer with 5 neurons (ReLU\n",
    "# as the activation function) and softmax as the activation function for the output.\n",
    "md2 = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Dense(5, input_dim = 4, activation = 'relu'),\n",
    "      tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    " # Use the stochastic descent gradient as the method to estimate the weights (optimizer = ’sgd’, \n",
    "# loss = ’categorical crossentropy’, and metrics = [’accuracy’]). Use epochs = 100 and \n",
    "    # batch size = 100 to build the model. \n",
    "md2.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "md2.fit(X_telecom_train,\n",
    "        tf.keras.utils.to_categorical(Y_telecom_train, num_classes = 2), \n",
    "        epochs = 100, \n",
    "        batch_size =100, \n",
    "        verbose = 0,\n",
    "        validation_data = (X_telecom_test, tf.keras.utils.to_categorical(Y_telecom_test, num_classes = 2)))\n",
    "        \n",
    "# using the model to predict on the test dataset. Using 10% as the cut-off value, compute the recall of this model. \n",
    "# Getting predictions from multi-layer perceptron (MLP) model on X_test dataset\n",
    "preds_2 = md2.predict(X_telecom_test)[:,1]\n",
    "\n",
    "# Defining likelyhoods to labels\n",
    "preds_2 = np.where(preds_2 < 0.10, 0, 1)\n",
    "# Report the average recall score across the 5-folds\n",
    "recall_score(Y_telecom_test, preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35dd87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the support vector machine (SVM) with kernel = 'rbf'\n",
    "from sklearn.svm import SVR, SVC\n",
    "\n",
    "# Building the svm with kernel = 'rbf'\n",
    "md3 = SVC(kernel = 'rbf', probability = True).fit(X_telecom_train, Y_telecom_train)\n",
    "\n",
    "# Predicting on the test dataset\n",
    "preds_3 = md3.predict_proba(X_telecom_test)[:,1]\n",
    "preds_3 = np.where(preds_3 < 0.10, 0, 1)\n",
    "recall_score(Y_telecom_test, preds_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78aea3b",
   "metadata": {},
   "source": [
    "#### I would choose model # to predict Churn because after all tests, model # has had the best performance so far."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
